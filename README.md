# TinyAutoGrad

TinyAutoGrad is an automatic differentiation engine and neural network library inspired by [Micrograd](https://github.com/karpathy/micrograd), with optional CUDA support.

---

## ğŸš€ Features

- âš™ï¸ Automatic differentiation (forward & backward)
- ğŸ§± Simple `Tensor` class with NumPy-style operations
- ğŸ§  Neural network support with MLP example
- ğŸ’» CPU and âš¡ CUDA backend support
- ğŸ§ª Built-in testing with `pytest`

---

## ğŸ› ï¸ Getting Started

### CPU (Default)

Run the MNIST example on CPU:

```bash
python -m samples.mnist.mnist
```

---

### CUDA (Experimental)

Ensure CUDA toolkit (`nvcc`) is installed:

```bash
# Compile CUDA backend
nvcc -shared -o libops.so tinyautograd/ops.cu -Xcompiler -fPIC -lcublas

# Run test.py
python test.py

# Run MNIST with CUDA backend
python -m samples.mnist.mnist_cuda
```

> âš ï¸ Make sure `libops.so` is in the current directory or Python load path.

---

## ğŸ§ª Running Tests

Run all unit tests:

```bash
pytest tinyautograd/ -s -v --cache-clear
```

Or a specific test:

```bash
pytest tinyautograd/test_rawtensor.py -s -v --cache-clear
```

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ libops.so
â”œâ”€â”€ test.py
â”œâ”€â”€ samples
â”‚   â””â”€â”€ mnist
â”‚       â”œâ”€â”€ mnist.py
â”‚       â”œâ”€â”€ mnist_cuda.py
â”‚       â”œâ”€â”€ train-images-idx3-ubyte.gz
â”‚       â””â”€â”€ train-labels-idx1-ubyte.gz
â”œâ”€â”€ tinyautograd
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ functional.py
â”‚   â”œâ”€â”€ nn.py
â”‚   â”œâ”€â”€ ops.cu
â”‚   â”œâ”€â”€ optim.py
â”‚   â”œâ”€â”€ rawtensor.py
â”‚   â”œâ”€â”€ tensor.py
â”‚   â”œâ”€â”€ test_nn.py
â”‚   â”œâ”€â”€ test_rawtensor.py
â”‚   â””â”€â”€ test_tensor.py
```

---

## ğŸ“„ License

MIT